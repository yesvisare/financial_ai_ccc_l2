{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 M9.1: Explainability & Citation Tracking\n",
    "\n",
    "**Learning Arc:**\n",
    "\n",
    "Financial RAG systems without explainability create regulatory liability. This notebook teaches you to build citation-tracked financial RAG with:\n",
    "\n",
    "1. **Source Attribution:** Inline citations [1], [2], [3] linking to specific SEC filings\n",
    "2. **Verifiable Citations:** Filing date, document section, direct quotes for audit verification\n",
    "3. **Retrieval Transparency:** Logs of retrieved documents, relevance scores, selection rationale\n",
    "4. **Audit Trail:** Immutable records meeting SOX Section 404 requirements\n",
    "5. **Conflict Detection:** Explicit disclosure when sources contradict\n",
    "6. **Citation Verification:** Post-generation validation catching LLM hallucinations\n",
    "\n",
    "**Prerequisites:**\n",
    "- Finance AI M7-M8 completed (RAG fundamentals)\n",
    "- Understanding of SEC filings (10-K, 10-Q, 8-K)\n",
    "- Basic compliance knowledge (SOX, SEC regulations)\n",
    "\n",
    "**By the end:**\n",
    "- Build explainable financial RAG systems\n",
    "- Implement citation tracking and verification\n",
    "- Create SOX-compliant audit trails\n",
    "- Validate citations and detect hallucinations\n",
    "\n",
    "**Estimated time:** 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OFFLINE Mode Configuration\n",
    "\n",
    "This notebook can run in two modes:\n",
    "- **OFFLINE:** Uses mock data, no API calls (for learning/testing)\n",
    "- **ONLINE:** Uses real APIs (ANTHROPIC, OPENAI, PINECONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Check service configuration\n",
    "ANTHROPIC_ENABLED = os.getenv(\"ANTHROPIC_ENABLED\", \"false\").lower() == \"true\"\n",
    "OPENAI_ENABLED = os.getenv(\"OPENAI_ENABLED\", \"false\").lower() == \"true\"\n",
    "PINECONE_ENABLED = os.getenv(\"PINECONE_ENABLED\", \"false\").lower() == \"true\"\n",
    "\n",
    "print(\"üîß Service Configuration:\")\n",
    "print(f\"  ANTHROPIC (Claude LLM):     {'‚úÖ Enabled' if ANTHROPIC_ENABLED else '‚ùå Disabled'}\")\n",
    "print(f\"  OPENAI (Embeddings):        {'‚úÖ Enabled' if OPENAI_ENABLED else '‚ùå Disabled'}\")\n",
    "print(f\"  PINECONE (Vector Database): {'‚úÖ Enabled' if PINECONE_ENABLED else '‚ùå Disabled'}\")\n",
    "print()\n",
    "\n",
    "if not any([ANTHROPIC_ENABLED, OPENAI_ENABLED, PINECONE_ENABLED]):\n",
    "    print(\"‚ö†Ô∏è Running in OFFLINE mode\")\n",
    "    print(\"   - Using mock data for demonstrations\")\n",
    "    print(\"   - No external API calls will be made\")\n",
    "    print(\"   - Expected outputs shown in comments\")\n",
    "    print()\n",
    "    print(\"To enable online mode:\")\n",
    "    print(\"   1. Copy .env.example to .env\")\n",
    "    print(\"   2. Add your API keys\")\n",
    "    print(\"   3. Set ANTHROPIC_ENABLED=true, OPENAI_ENABLED=true, PINECONE_ENABLED=true\")\n",
    "    print(\"   4. Restart Jupyter kernel\")\n",
    "else:\n",
    "    print(\"‚úÖ Running in ONLINE mode - live API calls enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:1**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Import Dependencies and Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Import our citation-tracking components\n",
    "from src.l3_m9_financial_compliance_risk import (\n",
    "    CitationAwareRetriever,\n",
    "    CitationMapBuilder,\n",
    "    CitationAwareLLMPrompt,\n",
    "    CitationVerificationEngine,\n",
    "    AuditTrailManager\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Successfully imported all components\")\n",
    "\n",
    "# Load example data\n",
    "with open('../example_data.json', 'r') as f:\n",
    "    example_data = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(example_data['queries'])} example queries\")\n",
    "print(\"\\nExample queries:\")\n",
    "for i, query in enumerate(example_data['queries'][:3], 1):\n",
    "    print(f\"  {i}. {query}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:2**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Component 1 - Citation-Aware Retrieval\n",
    "\n",
    "The first component retrieves financial documents and assigns citation markers [1], [2], [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = CitationAwareRetriever(\n",
    "    vectorstore=None,  # Will use mock data in offline mode\n",
    "    embeddings=None,\n",
    "    relevance_threshold=0.70\n",
    ")\n",
    "\n",
    "# Retrieve documents with citations\n",
    "query = \"What was Tesla's Q2 2024 free cash flow?\"\n",
    "print(f\"üìù Query: {query}\\n\")\n",
    "\n",
    "retrieval_result = retriever.retrieve_with_citations(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    filters={\"ticker\": \"TSLA\", \"fiscal_period\": \"Q2 2024\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retrieval Complete\\n\")\n",
    "print(f\"Documents retrieved: {retrieval_result['retrieval_log']['documents_retrieved']}\")\n",
    "print(f\"Documents used: {retrieval_result['retrieval_log']['documents_used']}\")\n",
    "print(f\"Documents excluded: {retrieval_result['retrieval_log']['documents_excluded']}\")\n",
    "print(f\"\\nCitation markers assigned: {list(retrieval_result['citation_map'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:3**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 3: Examine Citation Map\n",
    "\n",
    "Each citation has structured metadata for SEC audit verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first citation\n",
    "citation_map = retrieval_result['citation_map']\n",
    "\n",
    "print(\"üìã Citation [1] Metadata:\\n\")\n",
    "pprint(citation_map['[1]'])\n",
    "\n",
    "# Expected output shows:\n",
    "# - source_type: \"10-Q\"\n",
    "# - ticker: \"TSLA\"\n",
    "# - filing_date: \"2024-08-03\"\n",
    "# - fiscal_period: \"Q2 2024\"\n",
    "# - section: \"Financial Statements\"\n",
    "# - relevance_score: 0.92\n",
    "# - excerpt: Direct quote from filing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:4**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 4: Component 2 - LLM Prompting with Citation Instructions\n",
    "\n",
    "We construct a prompt that instructs the LLM to use citation markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = CitationAwareLLMPrompt()\n",
    "\n",
    "# Build context with citation markers\n",
    "context = \"\\n\\n\".join(retrieval_result['documents'])\n",
    "\n",
    "# Build RAG prompt\n",
    "llm_prompt = prompter.build_rag_prompt(\n",
    "    query=query,\n",
    "    retrieved_context=context,\n",
    "    citation_map=citation_map\n",
    ")\n",
    "\n",
    "print(\"üìù System Prompt (excerpt):\\n\")\n",
    "print(prompter.SYSTEM_PROMPT[:300] + \"...\\n\")\n",
    "\n",
    "print(\"üìù User Prompt (excerpt):\\n\")\n",
    "print(llm_prompt[:400] + \"...\")\n",
    "\n",
    "# Expected: Prompt includes citation markers [1], [2], [3]\n",
    "# and explicit instructions to cite every fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:5**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 5: Generate LLM Response (or Mock)\n",
    "\n",
    "In offline mode, we use a mock response. In online mode, we call Claude API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANTHROPIC_ENABLED:\n",
    "    # Online mode - call Claude API\n",
    "    from config import get_anthropic_client\n",
    "    \n",
    "    client = get_anthropic_client()\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        system=prompter.SYSTEM_PROMPT,\n",
    "        messages=[{\"role\": \"user\", \"content\": llm_prompt}]\n",
    "    )\n",
    "    llm_response = response.content[0].text\n",
    "    print(\"‚úÖ LLM response generated via Claude API\\n\")\n",
    "else:\n",
    "    # Offline mode - mock response\n",
    "    llm_response = \"\"\"Tesla reported Q2 2024 free cash flow of -$1.0B [1], primarily driven by $2.3B in capital expenditures for Gigafactory expansion [1]. \n",
    "\n",
    "However, operating cash flow improved to $1.3B compared to $0.5B in Q1 2024 [2], indicating operational efficiency gains. The negative free cash flow reflects strategic investments in manufacturing capacity rather than operational challenges.\n",
    "\n",
    "Management expects to achieve positive free cash flow in Q3 2024 as capital expenditures normalize [3].\"\"\"\n",
    "    print(\"‚ö†Ô∏è Using mock response (offline mode)\\n\")\n",
    "\n",
    "print(\"üí¨ LLM Response:\\n\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:6**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Component 3 - Citation Verification (Hallucination Detection)\n",
    "\n",
    "Verify that each citation actually supports the claim made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier = CitationVerificationEngine()\n",
    "\n",
    "verification = verifier.verify_citations(\n",
    "    response=llm_response,\n",
    "    citation_map=citation_map\n",
    ")\n",
    "\n",
    "print(\"üîç Verification Results:\\n\")\n",
    "print(f\"‚úÖ Verification passed: {verification['verification_passed']}\")\n",
    "print(f\"‚úÖ Verified claims: {len(verification['verified_claims'])}\")\n",
    "print(f\"‚ùå Unsupported claims: {len(verification['unsupported_claims'])}\")\n",
    "print(f\"üìä Overall fidelity: {verification['overall_fidelity']:.2%}\")\n",
    "print(f\"‚öñÔ∏è Recommendation: {verification['recommendation']}\")\n",
    "\n",
    "if verification['unsupported_claims']:\n",
    "    print(\"\\n‚ö†Ô∏è Unsupported claims detected:\")\n",
    "    for claim in verification['unsupported_claims'][:2]:  # Show first 2\n",
    "        print(f\"  - {claim['claim'][:100]}...\")\n",
    "        print(f\"    Similarity: {claim['similarity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:7**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 7: Component 4 - Audit Trail Creation\n",
    "\n",
    "Log complete pipeline for SOX Section 404 compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "audit_manager = AuditTrailManager()\n",
    "\n",
    "query_id = str(uuid.uuid4())\n",
    "response_id = audit_manager.log_complete_pipeline(\n",
    "    query_id=query_id,\n",
    "    user_id=\"analyst_demo\",\n",
    "    query_text=query,\n",
    "    retrieved_docs=[citation_map],\n",
    "    llm_response=llm_response,\n",
    "    citations=citation_map,\n",
    "    verification=verification\n",
    ")\n",
    "\n",
    "print(\"üìù Audit Trail Created:\\n\")\n",
    "print(f\"Query ID: {query_id}\")\n",
    "print(f\"Response ID: {response_id}\")\n",
    "print(f\"Audit entries logged: {len(audit_manager.audit_entries)}\")\n",
    "\n",
    "# Retrieve audit log\n",
    "logs = audit_manager.get_audit_log(query_id=query_id)\n",
    "print(\"\\n‚úÖ Audit log retrievable for SEC examination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:8**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 8: Examine Complete Audit Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_entry = logs[0]\n",
    "\n",
    "print(\"üìã Complete Audit Entry:\\n\")\n",
    "print(f\"Query ID: {audit_entry['query_id']}\")\n",
    "print(f\"User ID: {audit_entry['user_id']}\")\n",
    "print(f\"Timestamp: {audit_entry['timestamp']}\")\n",
    "print(f\"\\nQuery: {audit_entry['query_text']}\")\n",
    "print(f\"\\nDocuments retrieved: {audit_entry['retrieved_documents']['count']}\")\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  - Passed: {audit_entry['verification']['passed']}\")\n",
    "print(f\"  - Fidelity: {audit_entry['verification']['overall_fidelity']:.2%}\")\n",
    "print(f\"  - Recommendation: {audit_entry['verification']['recommendation']}\")\n",
    "\n",
    "# Expected: Complete pipeline logged\n",
    "# - Query, retrieval, response, citations, verification\n",
    "# - Timestamp for 7-year retention\n",
    "# - User attribution for accountability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:9**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 9: Testing Hallucination Detection\n",
    "\n",
    "Let's test the verification engine with a hallucinated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a response with hallucinated facts\n",
    "hallucinated_response = \"\"\"Tesla reported Q2 2024 free cash flow of $5.0B [1], \n",
    "the highest in company history [1]. Revenue grew 50% year-over-year [2].\"\"\"\n",
    "\n",
    "print(\"üß™ Testing hallucination detection...\\n\")\n",
    "print(\"Hallucinated response:\")\n",
    "print(hallucinated_response)\n",
    "print()\n",
    "\n",
    "hallucination_check = verifier.verify_citations(\n",
    "    response=hallucinated_response,\n",
    "    citation_map=citation_map\n",
    ")\n",
    "\n",
    "print(\"üîç Verification Results:\\n\")\n",
    "print(f\"‚úÖ Verification passed: {hallucination_check['verification_passed']}\")\n",
    "print(f\"üìä Fidelity: {hallucination_check['overall_fidelity']:.2%}\")\n",
    "print(f\"‚ùå Unsupported claims: {len(hallucination_check['unsupported_claims'])}\")\n",
    "\n",
    "if hallucination_check['unsupported_claims']:\n",
    "    print(\"\\n‚ö†Ô∏è Hallucinations detected:\")\n",
    "    for claim in hallucination_check['unsupported_claims']:\n",
    "        print(f\"  - Claim: {claim['claim'][:80]}...\")\n",
    "        print(f\"    Status: {claim['status']}\")\n",
    "        print(f\"    Similarity: {claim['similarity']:.2f}\\n\")\n",
    "\n",
    "# Expected: Verification catches hallucinations\n",
    "# Similarity scores will be low (<0.85 threshold)\n",
    "# Claims flagged for human review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:10**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 10: Testing Conflict Detection\n",
    "\n",
    "Demonstrate how to handle conflicting sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock conflicting sources\n",
    "conflict_response = \"\"\"Revenue shows mixed signals across sources: 10-Q reports 5% decline [1], \n",
    "while earnings call describes results as flat on constant currency basis [2], and analysts \n",
    "calculate 2% growth when adjusted for divestitures [3]. \n",
    "\n",
    "These discrepancies stem from different accounting adjustments and should be investigated \n",
    "before drawing conclusions.\"\"\"\n",
    "\n",
    "print(\"üìä Example of Proper Conflict Disclosure:\\n\")\n",
    "print(conflict_response)\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ This response properly discloses conflicting sources\")\n",
    "print(\"‚úÖ Cites all sources, not just favorable ones\")\n",
    "print(\"‚úÖ Explains why discrepancies exist\")\n",
    "print(\"‚úÖ Recommends investigation before conclusions\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è Improper: Cherry-picking only '2% growth' would constitute fraud risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:11**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 11: Production Deployment Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Production Deployment Checklist:\\n\")\n",
    "\n",
    "checklist = [\n",
    "    (\"Citation Accuracy\", \"Test on 100+ real SEC filings, achieve >95% accuracy\"),\n",
    "    (\"Audit Logging\", \"Verify all 5 components log correctly\"),\n",
    "    (\"Verification Threshold\", \"Calibrate semantic similarity (recommended: 0.85-0.90)\"),\n",
    "    (\"Monitoring\", \"Set up alerts for citation drift, verification failures\"),\n",
    "    (\"Compliance Documentation\", \"Prepare SOX 404 control documentation\"),\n",
    "    (\"Shadow Period\", \"2-week parallel operation with human validation\"),\n",
    "    (\"Quarterly Audits\", \"Recalibrate thresholds against manual review\"),\n",
    "]\n",
    "\n",
    "for i, (item, description) in enumerate(checklist, 1):\n",
    "    print(f\"{i}. {item}\")\n",
    "    print(f\"   {description}\\n\")\n",
    "\n",
    "print(\"\\nüìã Regulatory Framework:\")\n",
    "print(\"  - SEC Regulation S-P: Requires explainability for automated advice\")\n",
    "print(\"  - SOX Section 404: Requires audit trails (7-year retention)\")\n",
    "print(\"  - Investment Advisers Act: Fiduciary duty to clients\")\n",
    "print(\"  - GDPR Article 22: Right to explanation (EU clients)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVED_SECTION:12**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 12: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì What You've Learned:\\n\")\n",
    "print(\"‚úÖ 1. Citation-Aware Retrieval\")\n",
    "print(\"     - Retrieve documents with relevance scoring\")\n",
    "print(\"     - Assign citation markers [1], [2], [3]\")\n",
    "print(\"     - Filter by relevance threshold (0.70)\\n\")\n",
    "\n",
    "print(\"‚úÖ 2. Citation Map Generation\")\n",
    "print(\"     - Structured metadata for SEC audit\")\n",
    "print(\"     - Filing date, section, page number\")\n",
    "print(\"     - SHA256 hash for tamper detection\\n\")\n",
    "\n",
    "print(\"‚úÖ 3. LLM Prompting with Citations\")\n",
    "print(\"     - Instruct LLM to cite EVERY fact\")\n",
    "print(\"     - Embed citation markers in context\")\n",
    "print(\"     - Require disclosure if info unavailable\\n\")\n",
    "\n",
    "print(\"‚úÖ 4. Citation Verification\")\n",
    "print(\"     - Post-generation hallucination detection\")\n",
    "print(\"     - Semantic similarity checking (>0.85)\")\n",
    "print(\"     - Flag unsupported claims for review\\n\")\n",
    "\n",
    "print(\"‚úÖ 5. SOX-Compliant Audit Trail\")\n",
    "print(\"     - Immutable logging of complete pipeline\")\n",
    "print(\"     - 7-year retention support\")\n",
    "print(\"     - Query ‚Üí Retrieval ‚Üí Response ‚Üí Verification\\n\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\\n\")\n",
    "print(\"1. Test with real SEC filings from EDGAR API\")\n",
    "print(\"2. Configure ANTHROPIC, OPENAI, PINECONE for production\")\n",
    "print(\"3. Calibrate verification threshold with manual audits\")\n",
    "print(\"4. Set up PostgreSQL for immutable audit storage\")\n",
    "print(\"5. Conduct 2-week shadow period with compliance team\")\n",
    "print(\"6. Deploy with monitoring and quarterly audits\")\n",
    "\n",
    "print(\"\\nüìö Additional Resources:\")\n",
    "print(\"  - README.md: Complete documentation\")\n",
    "print(\"  - example_data.json: More query examples\")\n",
    "print(\"  - example_data.txt: SEC filing excerpts\")\n",
    "print(\"  - tests/: Comprehensive test suite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
