{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 M7.3: Financial Document Parsing & Chunking\n",
    "\n",
    "## Learning Arc\n",
    "\n",
    "1. Download SEC filings from EDGAR API\n",
    "2. Extract regulatory sections (Item 1, 1A, 7, 8)\n",
    "3. Parse XBRL financial data (200 core tags)\n",
    "4. Create compliance-aware chunks with metadata\n",
    "5. Tag chunks for temporal queries and audit trails\n",
    "\n",
    "## Concepts Covered\n",
    "\n",
    "- **SEC Filing Structure**: 10-K sections (Item 1, 1A, 7, 8) with mandated regulatory boundaries\n",
    "- **Compliance-Aware Chunking**: Preserve SOX Section 404 requirements while chunking\n",
    "- **XBRL Financial Data Parsing**: Extract structured financial data (balance sheet, income statement)\n",
    "- **Metadata Tagging**: Tag chunks for temporal queries (fiscal periods, tickers, sections)\n",
    "- **Audit Trail Generation**: SHA-256 hashes for compliance and tampering detection\n",
    "\n",
    "## By the end of this notebook, you'll have:\n",
    "\n",
    "✅ Working EDGAR API downloader with rate limiting  \n",
    "✅ Section parser preserving regulatory boundaries  \n",
    "✅ XBRL parser extracting 200 core financial tags  \n",
    "✅ Compliance-aware chunker with metadata  \n",
    "✅ Understanding of SOX Section 404 requirements  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OFFLINE Mode Check\n",
    "\n",
    "This notebook checks if EDGAR and other services are enabled. If disabled, it will run in OFFLINE mode with mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "EDGAR_ENABLED = os.getenv(\"EDGAR_ENABLED\", \"false\").lower() == \"true\"\n",
    "OPENAI_ENABLED = os.getenv(\"OPENAI_ENABLED\", \"false\").lower() == \"true\"\n",
    "PINECONE_ENABLED = os.getenv(\"PINECONE_ENABLED\", \"false\").lower() == \"true\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"L3 M7.3: Financial Document Parsing & Chunking\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "if not EDGAR_ENABLED:\n",
    "    print(\"⚠️  EDGAR service is DISABLED\")\n",
    "    print(\"⚠️  This notebook will run in OFFLINE mode\")\n",
    "    print(\"⚠️  Network calls will be skipped\")\n",
    "    print(\"⚠️  Mock data will be used for demonstrations\")\n",
    "    print()\n",
    "    print(\"To enable:\")\n",
    "    print(\"  1. Set EDGAR_ENABLED=true in .env\")\n",
    "    print(\"  2. Set SEC_USER_AGENT with company name + email\")\n",
    "    print(\"  3. Example: SEC_USER_AGENT='YourCompany yourteam@company.com'\")\n",
    "else:\n",
    "    print(\"✅ EDGAR service is ENABLED\")\n",
    "    print(\"✅ Notebook will make live API calls to SEC EDGAR\")\n",
    "    print(f\"✅ User-Agent: {os.getenv('SEC_USER_AGENT', 'Not set')}\")\n",
    "\n",
    "print()\n",
    "print(f\"OpenAI: {'✅ Enabled' if OPENAI_ENABLED else '⚠️  Disabled'}\")\n",
    "print(f\"Pinecone: {'✅ Enabled' if PINECONE_ENABLED else '⚠️  Disabled'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Setup\n",
    "\n",
    "Import all necessary libraries and initialize configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import logging\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Import our package components\n",
    "from src.l3_m7_financial_data_ingestion_compliance import (\n",
    "    EDGARDownloader,\n",
    "    SECFilingParser,\n",
    "    XBRLParser,\n",
    "    FinancialDocumentChunker,\n",
    "    chunk_filing,\n",
    "    extract_sections,\n",
    "    parse_xbrl_data\n",
    ")\n",
    "\n",
    "# Import configuration\n",
    "from config import (\n",
    "    EDGAR_ENABLED,\n",
    "    SEC_USER_AGENT,\n",
    "    CHUNK_SIZE,\n",
    "    CHUNK_OVERLAP,\n",
    "    get_config,\n",
    "    get_edgar_client,\n",
    "    get_openai_client,\n",
    "    get_pinecone_client\n",
    ")\n",
    "\n",
    "# Configure logging for notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "print(f\"   EDGAR: {EDGAR_ENABLED}\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"   Chunk overlap: {CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Initialize Components\n",
    "\n",
    "Create instances of our main classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration\n",
    "config = get_config()\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    if 'key' not in key.lower():  # Don't print API keys\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Initialize EDGAR downloader (if enabled)\n",
    "downloader = None\n",
    "if EDGAR_ENABLED and SEC_USER_AGENT:\n",
    "    try:\n",
    "        downloader = EDGARDownloader(SEC_USER_AGENT)\n",
    "        print(\"\\n✅ EDGAR downloader initialized\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n⚠️  EDGAR downloader initialization failed: {e}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  EDGAR downloader not initialized (service disabled or no User-Agent)\")\n",
    "\n",
    "# Initialize parser and chunker (always available)\n",
    "parser = SECFilingParser('10-K')\n",
    "xbrl_parser = XBRLParser()\n",
    "chunker = FinancialDocumentChunker(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "print(\"✅ Parser and chunker initialized\")\n",
    "print(f\"   Filing type: 10-K\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"   Overlap: {CHUNK_OVERLAP} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Download SEC Filing\n",
    "\n",
    "Download a sample 10-K filing from SEC EDGAR.\n",
    "\n",
    "**Note:** If EDGAR is disabled, this will use mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Microsoft 10-K\n",
    "ticker = 'MSFT'\n",
    "filing_type = '10-K'\n",
    "fiscal_year = 2023\n",
    "\n",
    "print(f\"Downloading {filing_type} for {ticker} (FY {fiscal_year})...\")\n",
    "\n",
    "if downloader:\n",
    "    # Real download from EDGAR\n",
    "    filing = downloader.download_filing(ticker, filing_type, fiscal_year)\n",
    "    print(\"✅ Downloaded from SEC EDGAR\")\n",
    "else:\n",
    "    # Mock data\n",
    "    filing = {\n",
    "        'html': downloader._get_mock_filing_html(ticker, filing_type, fiscal_year) if downloader else chunker._get_mock_html(ticker, filing_type),\n",
    "        'ticker': ticker,\n",
    "        'filing_type': filing_type,\n",
    "        'filing_date': '2023-07-27',\n",
    "        'accession_number': 'mock-0000789019-23-000090'\n",
    "    }\n",
    "    print(\"⚠️  Using mock data (EDGAR disabled)\")\n",
    "\n",
    "print(f\"\\nFiling metadata:\")\n",
    "print(f\"  Ticker: {filing['ticker']}\")\n",
    "print(f\"  Type: {filing['filing_type']}\")\n",
    "print(f\"  Date: {filing['filing_date']}\")\n",
    "print(f\"  Accession: {filing['accession_number']}\")\n",
    "print(f\"  HTML size: {len(filing['html'])} characters\")\n",
    "\n",
    "# Expected: Filing metadata with ticker, type, date, and HTML content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Extract Regulatory Sections\n",
    "\n",
    "Parse the HTML filing and extract regulatory sections (Item 1, 1A, 7, 8).\n",
    "\n",
    "**Key Point:** Section boundaries are preserved for SOX Section 404 compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sections from HTML\n",
    "print(\"Extracting regulatory sections...\")\n",
    "sections = parser.extract_sections(filing['html'])\n",
    "\n",
    "print(f\"\\n✅ Extracted {len(sections)} sections:\")\n",
    "for section_name, section_content in sections.items():\n",
    "    print(f\"\\n{section_name}:\")\n",
    "    print(f\"  Length: {len(section_content)} characters\")\n",
    "    print(f\"  Preview: {section_content[:150]}...\")\n",
    "\n",
    "# Expected: Dictionary with Item 1, Item 1A, Item 7, Item 8 sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Parse XBRL Financial Data\n",
    "\n",
    "Extract structured financial data from Item 8 (Financial Statements).\n",
    "\n",
    "**XBRL Tags:** Standardized tags like `us-gaap:Assets`, `us-gaap:Revenues`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse XBRL data from Item 8\n",
    "if 'Item 8' in sections:\n",
    "    print(\"Parsing XBRL financial data from Item 8...\")\n",
    "    xbrl_data = xbrl_parser.parse_xbrl_from_html(sections['Item 8'])\n",
    "\n",
    "    print(\"\\n✅ XBRL data extracted:\")\n",
    "    print(f\"\\nBalance Sheet:\")\n",
    "    for tag, value in xbrl_data['balance_sheet'].items():\n",
    "        print(f\"  {tag}: {value}\")\n",
    "\n",
    "    print(f\"\\nIncome Statement:\")\n",
    "    for tag, value in xbrl_data['income_statement'].items():\n",
    "        print(f\"  {tag}: {value}\")\n",
    "\n",
    "    print(f\"\\nFiscal Period: {xbrl_data['fiscal_period']}\")\n",
    "else:\n",
    "    print(\"⚠️  Item 8 not found in sections\")\n",
    "    xbrl_data = {}\n",
    "\n",
    "# Expected: Structured financial data with balance sheet and income statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Create Compliance-Aware Chunks\n",
    "\n",
    "Chunk the filing while preserving:\n",
    "1. Regulatory section boundaries (no cross-section chunks)\n",
    "2. Financial table integrity (tables not split)\n",
    "3. Context through overlap (200 chars between chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the filing\n",
    "print(f\"Creating compliance-aware chunks...\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"  Overlap: {CHUNK_OVERLAP} chars\")\n",
    "\n",
    "user_agent = SEC_USER_AGENT if EDGAR_ENABLED else None\n",
    "chunks = chunker.chunk_filing(ticker, filing_type, fiscal_year, user_agent)\n",
    "\n",
    "print(f\"\\n✅ Created {len(chunks)} chunks\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk #{i+1}:\")\n",
    "    print(f\"  Section: {chunk['metadata']['section']}\")\n",
    "    print(f\"  Length: {len(chunk['text'])} chars\")\n",
    "    print(f\"  Hash: {chunk['metadata']['chunk_hash']}\")\n",
    "    print(f\"  Text preview: {chunk['text'][:100]}...\")\n",
    "\n",
    "# Expected: List of chunks with metadata (section, ticker, fiscal_period, hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Examine Chunk Metadata\n",
    "\n",
    "Each chunk includes complete metadata for audit trails and temporal queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine metadata structure\n",
    "sample_chunk = chunks[0]\n",
    "\n",
    "print(\"Chunk metadata structure:\")\n",
    "print(f\"\\nText field:\")\n",
    "print(f\"  Type: {type(sample_chunk['text'])}\")\n",
    "print(f\"  Length: {len(sample_chunk['text'])} chars\")\n",
    "\n",
    "print(f\"\\nMetadata fields:\")\n",
    "for key, value in sample_chunk['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Count chunks by section\n",
    "section_counts = {}\n",
    "for chunk in chunks:\n",
    "    section = chunk['metadata']['section']\n",
    "    section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "print(f\"\\nChunks per section:\")\n",
    "for section, count in section_counts.items():\n",
    "    print(f\"  {section}: {count} chunks\")\n",
    "\n",
    "# Expected: Complete metadata with ticker, section, fiscal_period, hash, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Financial Statement Chunks\n",
    "\n",
    "Examine how financial statements (Item 8) are chunked.\n",
    "\n",
    "**Key Point:** Tables are treated as atomic units (not split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find financial statement chunks\n",
    "financial_chunks = [c for c in chunks if 'Item 8' in c['metadata']['section']]\n",
    "\n",
    "print(f\"Financial statement chunks: {len(financial_chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(financial_chunks[:2]):  # Show first 2\n",
    "    print(f\"\\nFinancial Chunk #{i+1}:\")\n",
    "    print(f\"  Table type: {chunk['metadata'].get('table_type', 'narrative')}\")\n",
    "    print(f\"  Sensitivity: {chunk['metadata']['sensitivity']}\")\n",
    "    print(f\"  Fiscal period: {chunk['metadata'].get('fiscal_period', 'N/A')}\")\n",
    "    print(f\"  Length: {len(chunk['text'])} chars\")\n",
    "    print(f\"  Preview: {chunk['text'][:200]}...\")\n",
    "\n",
    "# Expected: Financial chunks with table_type metadata (balance_sheet, income_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Verify Compliance Requirements\n",
    "\n",
    "Verify that chunks meet SOX Section 404 requirements:\n",
    "1. Section boundaries preserved (no cross-section chunks)\n",
    "2. Tables not split (each table = one chunk)\n",
    "3. Audit trail (SHA-256 hashes)\n",
    "4. Complete metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compliance Verification:\\n\")\n",
    "\n",
    "# 1. Check section boundary preservation\n",
    "sections_found = set(c['metadata']['section'] for c in chunks)\n",
    "print(f\"✅ Section boundary preservation:\")\n",
    "print(f\"   Sections found: {len(sections_found)}\")\n",
    "print(f\"   Sections: {', '.join(sorted(sections_found))}\")\n",
    "\n",
    "# 2. Check all chunks have hashes\n",
    "chunks_with_hash = sum(1 for c in chunks if 'chunk_hash' in c['metadata'])\n",
    "print(f\"\\n✅ Audit trail (hashes):\")\n",
    "print(f\"   Chunks with hash: {chunks_with_hash}/{len(chunks)}\")\n",
    "print(f\"   Sample hash: {chunks[0]['metadata']['chunk_hash']}\")\n",
    "\n",
    "# 3. Check required metadata fields\n",
    "required_fields = ['ticker', 'filing_type', 'section', 'filing_date', \n",
    "                   'accession_number', 'sensitivity', 'chunk_hash', 'created_at']\n",
    "missing_fields = []\n",
    "for chunk in chunks:\n",
    "    for field in required_fields:\n",
    "        if field not in chunk['metadata']:\n",
    "            missing_fields.append(field)\n",
    "\n",
    "if not missing_fields:\n",
    "    print(f\"\\n✅ Metadata completeness:\")\n",
    "    print(f\"   All {len(required_fields)} required fields present in all chunks\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Missing metadata fields: {set(missing_fields)}\")\n",
    "\n",
    "# 4. Check financial tables not split\n",
    "financial_chunks = [c for c in chunks if c['metadata'].get('table_type')]\n",
    "print(f\"\\n✅ Financial table integrity:\")\n",
    "print(f\"   Financial table chunks: {len(financial_chunks)}\")\n",
    "print(f\"   Tables treated as atomic units (not split)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SOX Section 404 compliance verified ✅\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Optional - Vector Database Integration\n",
    "\n",
    "If OpenAI and Pinecone are enabled, this section demonstrates:\n",
    "1. Generating embeddings for chunks\n",
    "2. Storing in Pinecone vector database\n",
    "3. Querying with semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vector DB features are available\n",
    "if OPENAI_ENABLED and PINECONE_ENABLED:\n",
    "    print(\"Setting up vector database integration...\\n\")\n",
    "\n",
    "    # Get clients\n",
    "    openai_client = get_openai_client()\n",
    "    pinecone_index = get_pinecone_client()\n",
    "\n",
    "    if openai_client and pinecone_index:\n",
    "        print(\"✅ OpenAI and Pinecone clients initialized\")\n",
    "\n",
    "        # Generate embeddings for first 5 chunks (demo)\n",
    "        print(\"\\nGenerating embeddings for first 5 chunks...\")\n",
    "        for i, chunk in enumerate(chunks[:5]):\n",
    "            # Generate embedding\n",
    "            response = openai_client.embeddings.create(\n",
    "                model='text-embedding-3-small',\n",
    "                input=chunk['text']\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "\n",
    "            # Upsert to Pinecone\n",
    "            chunk_id = f\"{chunk['metadata']['ticker']}-{chunk['metadata']['filing_type']}-chunk-{i}\"\n",
    "            pinecone_index.upsert(vectors=[{\n",
    "                'id': chunk_id,\n",
    "                'values': embedding,\n",
    "                'metadata': chunk['metadata']\n",
    "            }])\n",
    "\n",
    "            print(f\"  Chunk {i+1}: {chunk['metadata']['section'][:30]}... → Pinecone\")\n",
    "\n",
    "        print(f\"\\n✅ {5} chunks indexed in Pinecone\")\n",
    "    else:\n",
    "        print(\"⚠️  Client initialization failed\")\n",
    "else:\n",
    "    print(\"⚠️  Vector database features disabled\")\n",
    "    print(\"   To enable: Set OPENAI_ENABLED=true and PINECONE_ENABLED=true in .env\")\n",
    "\n",
    "# Expected: Embeddings generated and stored in Pinecone (if enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Query Example\n",
    "\n",
    "If vector DB is enabled, demonstrate semantic search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_ENABLED and PINECONE_ENABLED:\n",
    "    openai_client = get_openai_client()\n",
    "    pinecone_index = get_pinecone_client()\n",
    "\n",
    "    if openai_client and pinecone_index:\n",
    "        # Example query\n",
    "        query_text = \"What are Microsoft's total assets?\"\n",
    "        print(f\"Query: {query_text}\\n\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_response = openai_client.embeddings.create(\n",
    "            model='text-embedding-3-small',\n",
    "            input=query_text\n",
    "        )\n",
    "        query_embedding = query_response.data[0].embedding\n",
    "\n",
    "        # Search Pinecone\n",
    "        results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=3,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        print(f\"Top {len(results['matches'])} results:\\n\")\n",
    "        for i, match in enumerate(results['matches']):\n",
    "            print(f\"Result {i+1}:\")\n",
    "            print(f\"  Score: {match['score']:.4f}\")\n",
    "            print(f\"  Section: {match['metadata']['section']}\")\n",
    "            print(f\"  Chunk ID: {match['id']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"⚠️  Clients not available\")\n",
    "else:\n",
    "    print(\"⚠️  Vector search requires OpenAI and Pinecone\")\n",
    "    print(\"   This is an optional enhancement feature\")\n",
    "\n",
    "# Expected: Top 3 relevant chunks for the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ✅ **Download SEC filings** from EDGAR API with rate limiting compliance\n",
    "2. ✅ **Extract regulatory sections** (Item 1, 1A, 7, 8) while preserving boundaries\n",
    "3. ✅ **Parse XBRL financial data** from standardized tags\n",
    "4. ✅ **Create compliance-aware chunks** that preserve SOX Section 404 requirements\n",
    "5. ✅ **Generate audit trails** with SHA-256 hashes for each chunk\n",
    "6. ✅ **Tag chunks with metadata** for temporal queries and compliance reporting\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Section boundaries must be preserved** - Don't split regulatory sections mid-content\n",
    "- **Financial tables are atomic** - Balance sheets, income statements must stay intact\n",
    "- **Chunk overlap is critical** - Prevents context loss across boundaries (15-20% overlap)\n",
    "- **Metadata enables compliance** - Fiscal periods, tickers, sections required for audit trails\n",
    "- **Rate limiting is enforced** - SEC blocks IPs exceeding 10 req/sec (not optional)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Process multiple companies' filings in batch\n",
    "2. Implement cross-company comparative queries\n",
    "3. Add material event monitoring (8-K filings)\n",
    "4. Build audit-ready reporting system\n",
    "\n",
    "See the **README.md** for the complete Decision Card and Common Failures reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
